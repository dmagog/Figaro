# data_loader.py
from sqlmodel import Session, select, delete
from models import Hall, HallTransition, Concert, Artist, Author, Composition, ConcertArtistLink, ConcertCompositionLink, Purchase, AvailableRoute, OffProgram, EventFormat, Genre, ConcertGenreLink
from models.statistics import Statistics
from datetime import datetime, timedelta, timezone
import pandas as pd
from typing import Dict, List
import logging
import csv
from sqlmodel import Session
from models.route import Route
from sqlalchemy import and_, or_, text
import re
from . import route_service

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –±–∞—Ç—á–∏–Ω–≥–∞
import os
BATCH_SIZE = int(os.getenv('DATA_LOADER_BATCH_SIZE', '1000'))

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è/–≤–∫–ª—é—á–µ–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π (–¥–ª—è PostgreSQL)
def disable_foreign_keys(session):
    """–û—Ç–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏"""
    try:
        session.exec(text("SET session_replication_role = replica;"))
        session.commit()
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫–ª—é—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏: {e}")

def enable_foreign_keys(session):
    """–í–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π"""
    try:
        session.exec(text("SET session_replication_role = DEFAULT;"))
        session.commit()
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏: {e}")


def get_or_create(session, model, **kwargs):
    instance = session.exec(select(model).filter_by(**kwargs)).first()
    if instance:
        return instance
    instance = model(**kwargs)
    session.add(instance)
    session.commit()
    session.refresh(instance)
    return instance


def bulk_get_or_create(session, model, records: List[Dict], unique_fields: List[str]):
    """–ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ"""
    if not records:
        return {}
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
    unique_keys = set()
    for record in records:
        key = tuple(record[field] for field in unique_fields if field in record)
        unique_keys.add(key)
    
    # –ò—â–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
    existing_map = {}
    if unique_keys:
        # –°–æ–∑–¥–∞–µ–º OR —É—Å–ª–æ–≤–∏–µ –¥–ª—è –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π
        conditions = []
        for key in unique_keys:
            key_dict = dict(zip(unique_fields, key))
            subconds = [(getattr(model, field) == value) for field, value in key_dict.items()]
            if subconds:
                condition = and_(*subconds)
                conditions.append(condition)
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π —á–µ—Ä–µ–∑ OR
        if conditions:
            query = select(model).where(or_(*conditions))
            existing_records = session.exec(query).all()
            for record in existing_records:
                key = tuple(getattr(record, field) for field in unique_fields)
                existing_map[key] = record
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
    new_records = []
    for record in records:
        key = tuple(record[field] for field in unique_fields if field in record)
        if key not in existing_map:
            new_records.append(record)
    
    if new_records:
        # Bulk insert –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        session.add_all([model(**record) for record in new_records])
        session.commit()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ä—Ç—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        for record in new_records:
            key = tuple(record[field] for field in unique_fields if field in record)
            instance = session.exec(
                select(model).where(
                    *[getattr(model, field) == record[field] for field in unique_fields if field in record]
                )
            ).first()
            existing_map[key] = instance
    
    return existing_map


def load_halls(session: Session, df_halls: pd.DataFrame):
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º {len(df_halls)} –∑–∞–ª–æ–≤ –∏–∑ Excel")
    
    records = []
    for _, row in df_halls.iterrows():
        records.append({
            "name": row["HallName"],
            "concert_count": row.get("count", 0),
            "address": None if pd.isna(row.get("Adress")) else row.get("Adress"),
            "latitude": None if pd.isna(row.get("Lat")) else row.get("Lat"),
            "longitude": None if pd.isna(row.get("Lon")) else row.get("Lon"),
            "seats": 0 if pd.isna(row.get("Seats")) else row.get("Seats"),
        })
    
    bulk_get_or_create(session, Hall, records, ["name"])
    
    count = session.exec(select(Hall)).all()
    logger.info(f"–í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å {len(count)} –∑–∞–ª–æ–≤")


def load_concerts(session: Session, df_concerts: pd.DataFrame):
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º {len(df_concerts)} –∫–æ–Ω—Ü–µ—Ä—Ç–æ–≤ –∏–∑ Excel")
    
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∑–∞–ª—ã –≤ –ø–∞–º—è—Ç—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    halls = {hall.name: hall for hall in session.exec(select(Hall)).all()}
    
    records = []
    for _, row in df_concerts.iterrows():
        hall = halls.get(row["HallName"])
        if not hall:
            logger.warning(f"–ó–∞–ª {row['HallName']} –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–Ω—Ü–µ—Ä—Ç {row['ShowId']}")
            continue
            
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º tickets_left –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –º–µ—Å—Ç –≤ –∑–∞–ª–µ, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        tickets_left = None
        if not pd.isna(row.get("TicketsLeft")):
            tickets_left = int(row.get("TicketsLeft"))
        elif hall.seats > 0:
            tickets_left = hall.seats
            
        records.append({
            "external_id": row["ShowId"],
            "name": row["ShowName"],
            "datetime": pd.to_datetime(row["ShowDate"]),
            "duration": pd.to_timedelta(row["ShowLong"]),
            "genre": None if pd.isna(row.get("Genre")) else row.get("Genre"),
            "price": None if pd.isna(row.get("Price")) else row.get("Price"),
            "is_family_friendly": bool(row.get("Family", False)),
            "tickets_available": bool(row.get("Tickets", False)),
            "tickets_left": tickets_left,
            "link": None if pd.isna(row.get("link")) else row.get("link"),
            "hall_id": hall.id,
        })
    
    bulk_get_or_create(session, Concert, records, ["external_id"])
    
    count = session.exec(select(Concert)).all()
    logger.info(f"–í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å {len(count)} –∫–æ–Ω—Ü–µ—Ä—Ç–æ–≤")


def load_artists(session: Session, df_artists: pd.DataFrame):
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –∞—Ä—Ç–∏—Å—Ç–æ–≤ –∏–∑ Excel")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ü–µ—Ä—Ç—ã –≤ –ø–∞–º—è—Ç—å
    concerts = {concert.external_id: concert.id for concert in session.exec(select(Concert)).all()}
    
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏—Å—Ç–æ–≤
    unique_artists = df_artists["Artists"].dropna().unique()
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_artists)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏—Å—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å–∏ –∞—Ä—Ç–∏—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Ñ–ª–∞–≥–∞ is_special
    artist_records = []
    for artist_name in unique_artists:
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∞—Ä—Ç–∏—Å—Ç–∞, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å is_special
        artist_rows = df_artists[df_artists["Artists"] == artist_name]
        # –°—Ç—Ä–æ–≥–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—á–∏—Ç–∞—é—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏
        is_special = any(
            bool(val) and str(val).strip().lower() not in ["false", "0", "nan", "none", ""]
            for val in artist_rows["Spetial"] if not pd.isna(val)
        )
    
        artist_records.append({
            "name": artist_name,
            "is_special": is_special
            })
    
    # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ç–∏—Å—Ç–æ–≤
    artists_map = bulk_get_or_create(session, Artist, artist_records, ["name"])
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∞—Ä—Ç–∏—Å—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    artists_dict = {artist.name: artist for artist in session.exec(select(Artist)).all()}
    
    # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑–∏ —Å –∫–æ–Ω—Ü–µ—Ä—Ç–∞–º–∏
    for _, row in df_artists.drop_duplicates(["ShowNum", "Artists"]).iterrows():
        concert_id = concerts.get(row["ShowNum"])
        if concert_id:
            artist_name = row["Artists"]
            artist = artists_dict.get(artist_name)
            
        if artist:
            get_or_create(
                session,
                ConcertArtistLink,
                concert_id=concert_id,
                artist_id=artist.id
            )
    
    count = session.exec(select(Artist)).all()
    logger.info(f"–í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å {len(count)} –∞—Ä—Ç–∏—Å—Ç–æ–≤")


def load_compositions(session: Session, df_details: pd.DataFrame):
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏–∑ Excel")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ü–µ—Ä—Ç—ã –≤ –ø–∞–º—è—Ç—å
    concerts = {concert.external_id: concert.id for concert in session.exec(select(Concert)).all()}
    
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤
    unique_authors = df_details["Author"].dropna().unique()
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_authors)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤")
    
    author_records = []
    for author_name in unique_authors:
        author_records.append({"name": author_name})
    
    # –°–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤
    authors_map = bulk_get_or_create(session, Author, author_records, ["name"])
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    authors_dict = {author.name: author for author in session.exec(select(Author)).all()}
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å–æ—á–µ—Ç–∞–Ω–∏—è–º (Author, Programm)
    composition_records = []
    link_records = []
    
    for _, row in df_details.drop_duplicates(["Author", "Programm"]).iterrows():
        author_name = row["Author"]
        author = authors_dict.get(author_name)
        
        if author:
            composition_records.append({
                "name": row["Programm"],
                "author_id": author.id
            })
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
    for comp_record in composition_records:
        get_or_create(session, Composition, **comp_record)
    
    # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑–∏ —Å –∫–æ–Ω—Ü–µ—Ä—Ç–∞–º–∏
    for _, row in df_details.drop_duplicates(["ShowNum", "Author", "Programm"]).iterrows():
        concert_id = concerts.get(row["ShowNum"])
        if concert_id:
            author_name = row["Author"]
            composition_name = row["Programm"]
            
        composition = session.exec(
            select(Composition)
            .join(Author)
            .where(
                    Composition.name == composition_name,
                    Author.name == author_name
            )
        ).first()
        
        if composition:
            get_or_create(
                session,
                ConcertCompositionLink,
                concert_id=concert_id,
                composition_id=composition.id
            )

    count = session.exec(select(Composition)).all()
    logger.info(f"–í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å {len(count)} –∫–æ–º–ø–æ–∑–∏—Ü–∏–π")


def load_purchases(session: Session, df_ops: pd.DataFrame):
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º {len(df_ops.drop_duplicates(['OpId']))} –ø–æ–∫—É–ø–æ–∫ –∏–∑ Excel")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ü–µ—Ä—Ç—ã –≤ –ø–∞–º—è—Ç—å
    concerts = {concert.external_id: concert.id for concert in session.exec(select(Concert)).all()}
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ–∫—É–ø–∫–∏ –ø–æ –±–∞—Ç—á–∞–º
    total_records = len(df_ops.drop_duplicates(["OpId"]))
    processed = 0
    
    for batch_start in range(0, total_records, BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE, total_records)
        batch_df = df_ops.drop_duplicates(["OpId"]).iloc[batch_start:batch_end]
        
        records = []
        for _, row in batch_df.iterrows():
            concert_id = concerts.get(row["ShowId"])
            if not concert_id:
                logger.warning(f"–ö–æ–Ω—Ü–µ—Ä—Ç {row['ShowId']} –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∫—É–ø–∫—É {row['OpId']}")
                continue
                
            records.append({
                "external_op_id": row["OpId"],
                "user_external_id": str(row["ClientId"]),
                "concert_id": concert_id,
                "purchased_at": pd.to_datetime(row["OpDate"]),
                "price": None if pd.isna(row.get("Price")) else row.get("Price")
            })
        
        if records:
            bulk_get_or_create(session, Purchase, records, ["external_op_id"])
        
        processed += len(records)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{total_records} –ø–æ–∫—É–ø–æ–∫ ({processed/total_records*100:.1f}%)")
    
    count = session.exec(select(Purchase)).all()
    logger.info(f"–í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å {len(count)} –ø–æ–∫—É–ø–æ–∫")


def update_customer_route_matches(session: Session):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π —Å –º–∞—Ä—à—Ä—É—Ç–∞–º–∏.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞—Ä—à—Ä—É—Ç–æ–≤.
    """
    from models import CustomerRouteMatch
    from datetime import datetime
    from collections import defaultdict
    
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π —Å –º–∞—Ä—à—Ä—É—Ç–∞–º–∏...")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–∞—Ä—à—Ä—É—Ç—ã
    all_routes = session.exec(select(Route)).all()
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_routes)} –º–∞—Ä—à—Ä—É—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –º–∞—Ä—à—Ä—É—Ç–æ–≤
    routes_by_composition = {}
    routes_by_length = defaultdict(list)  # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–∞—Ä—à—Ä—É—Ç—ã –ø–æ –¥–ª–∏–Ω–µ
    routes_by_concerts = defaultdict(list)  # –ò–Ω–¥–µ–∫—Å –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–æ–Ω—Ü–µ—Ä—Ç–∞–º
    
    for route in all_routes:
        try:
            route_concert_ids = tuple(sorted([int(x.strip()) for x in route.Sostav.split(',') if x.strip()]))
            routes_by_composition[route_concert_ids] = route
            routes_by_length[len(route_concert_ids)].append((route_concert_ids, route))
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–æ–Ω—Ü–µ—Ä—Ç–∞–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            for concert_id in route_concert_ids:
                routes_by_concerts[concert_id].append((route_concert_ids, route))
        except (ValueError, AttributeError) as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –º–∞—Ä—à—Ä—É—Ç–∞ {route.id}: {e}")
            continue
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ –∏–Ω–¥–µ–∫—Å–æ–≤: {len(routes_by_composition)} –º–∞—Ä—à—Ä—É—Ç–æ–≤, {len(routes_by_concerts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ—Ä—Ç–æ–≤")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π —Å –∏—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ—Ä—Ç–∞–º–∏
    from sqlalchemy import func
    customer_concerts = session.exec(
        select(
            Purchase.user_external_id,
            func.array_agg(func.distinct(Purchase.concert_id)).label('unique_concert_ids')
        )
        .group_by(Purchase.user_external_id)
    ).all()
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(customer_concerts)} –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è")
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    session.exec(delete(CustomerRouteMatch))
    session.commit()
    logger.info("–û—á–∏—â–µ–Ω—ã —Å—Ç–∞—Ä—ã–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è")
    
    # –ë–∞—Ç—á–∏–Ω–≥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    BATCH_SIZE = 500
    match_records = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–≥–æ –ø–æ–∫—É–ø–∞—Ç–µ–ª—è
    processed = 0
    for customer_data in customer_concerts:
        external_id = str(customer_data[0])
        unique_concert_ids = customer_data[1] if customer_data[1] else []
        
        if not unique_concert_ids:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –ø–æ–∫—É–ø–∞—Ç–µ–ª—è –±–µ–∑ –ø–æ–∫—É–ø–æ–∫
            match_record = CustomerRouteMatch(
                user_external_id=external_id,
                found=False,
                match_type="none",
                reason="–£ –ø–æ–∫—É–ø–∞—Ç–µ–ª—è –Ω–µ—Ç –ø–æ–∫—É–ø–æ–∫",
                customer_concerts="",
                customer_concerts_count=0,
                total_routes_checked=len(all_routes),
                updated_at=datetime.utcnow()
            )
            match_records.append(match_record)
            processed += 1
            continue
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ—Ä—Ç—ã
        customer_concert_ids = sorted(unique_concert_ids)
        customer_concert_ids_str = ",".join(map(str, customer_concert_ids))
        customer_concert_ids_tuple = tuple(customer_concert_ids)
        customer_concert_ids_set = set(customer_concert_ids)
        
        # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        exact_matches = []
        partial_matches = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (O(1) –æ–ø–µ—Ä–∞—Ü–∏—è)
        if customer_concert_ids_tuple in routes_by_composition:
            route = routes_by_composition[customer_concert_ids_tuple]
            exact_matches.append({
                "route_id": route.id,
                "match_type": "exact",
                "match_percentage": 100.0
            })
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        if not exact_matches:
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã –ø–æ –ø–µ—Ä–≤–æ–º—É –∫–æ–Ω—Ü–µ—Ä—Ç—É
            potential_routes = []
            if customer_concert_ids:
                first_concert = customer_concert_ids[0]
                if first_concert in routes_by_concerts:
                    potential_routes.extend(routes_by_concerts[first_concert])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã
            for route_concert_ids_tuple, route in potential_routes:
                if customer_concert_ids_set.issubset(set(route_concert_ids_tuple)):
                    match_percentage = (len(customer_concert_ids) / len(route_concert_ids_tuple)) * 100
                    partial_matches.append({
                        "route_id": route.id,
                        "match_type": "partial",
                        "match_percentage": match_percentage
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            partial_matches.sort(key=lambda x: x["match_percentage"], reverse=True)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –º–∞—Ä—à—Ä—É—Ç
        best_match = None
        match_type = "none"
        reason = "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤"
        
        if exact_matches:
            best_match = exact_matches[0]
            match_type = "exact"
            reason = None
        elif partial_matches:
            best_match = partial_matches[0]
            match_type = "partial"
            reason = None
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å
        match_record = CustomerRouteMatch(
            user_external_id=external_id,
            found=best_match is not None,
            match_type=match_type,
            reason=reason,
            customer_concerts=customer_concert_ids_str,
            customer_concerts_count=len(customer_concert_ids),
            best_route_id=best_match["route_id"] if best_match else None,
            match_percentage=best_match["match_percentage"] if best_match else None,
            total_routes_checked=len(all_routes),
            updated_at=datetime.utcnow()
        )
        
        match_records.append(match_record)
        processed += 1
        
        # –ë–∞—Ç—á–∏–Ω–≥: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å–∏ –ø–æ—Ä—Ü–∏—è–º–∏
        if len(match_records) >= BATCH_SIZE:
            session.add_all(match_records)
            session.commit()
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{len(customer_concerts)} –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π (–±–∞—Ç—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω)")
            match_records = []
        
        if processed % 1000 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{len(customer_concerts)} –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–ø–∏—Å–∏
    if match_records:
        session.add_all(match_records)
        session.commit()
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–∞—Ç—á –∏–∑ {len(match_records)} –∑–∞–ø–∏—Å–µ–π")
    
    logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed} –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π")


def load_genres(session: Session):
    """
    –°–æ–∑–¥–∞–µ—Ç –∂–∞–Ω—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ—Ä—Ç–æ–≤ –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏—Ö
    """
    logger.info("–°–æ–∑–¥–∞–µ–º –∂–∞–Ω—Ä—ã –∏ —Å–≤—è–∑—ã–≤–∞–µ–º —Å –∫–æ–Ω—Ü–µ—Ä—Ç–∞–º–∏...")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–æ–Ω—Ü–µ—Ä—Ç—ã —Å –∂–∞–Ω—Ä–∞–º–∏
        concerts = session.exec(select(Concert).where(Concert.genre.is_not(None))).all()
        
        if not concerts:
            logger.info("–ù–µ—Ç –∫–æ–Ω—Ü–µ—Ä—Ç–æ–≤ —Å –∂–∞–Ω—Ä–∞–º–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∂–∞–Ω—Ä—ã
        unique_genres = set()
        for concert in concerts:
            if concert.genre:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∂–∞–Ω—Ä –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ (–Ω–µ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º)
                unique_genres.add(concert.genre.strip())
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_genres)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –∂–∞–Ω—Ä—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        genre_map = {}  # –∏–º—è –∂–∞–Ω—Ä–∞ -> –æ–±—ä–µ–∫—Ç Genre
        for genre_name in unique_genres:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π –∂–∞–Ω—Ä
            existing_genre = session.exec(
                select(Genre).where(Genre.name == genre_name)
            ).first()
            
            if existing_genre:
                genre_map[genre_name] = existing_genre
                logger.debug(f"–ñ–∞–Ω—Ä '{genre_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (ID: {existing_genre.id})")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∂–∞–Ω—Ä
                new_genre = Genre(name=genre_name)
                session.add(new_genre)
                session.commit()
                session.refresh(new_genre)
                genre_map[genre_name] = new_genre
                logger.info(f"–°–æ–∑–¥–∞–Ω –∂–∞–Ω—Ä '{genre_name}' (ID: {new_genre.id})")
        
        # –°–≤—è–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ü–µ—Ä—Ç—ã —Å –∂–∞–Ω—Ä–∞–º–∏
        links_created = 0
        for concert in concerts:
            if concert.genre:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∂–∞–Ω—Ä –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ
                genre_name = concert.genre.strip()
                
                if genre_name in genre_map:
                    genre = genre_map[genre_name]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Å–≤—è–∑—å
                    existing_link = session.exec(
                        select(ConcertGenreLink).where(
                            ConcertGenreLink.concert_id == concert.id,
                            ConcertGenreLink.genre_id == genre.id
                        )
                    ).first()
                    
                    if not existing_link:
                        # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑—å
                        link = ConcertGenreLink(
                            concert_id=concert.id,
                            genre_id=genre.id
                        )
                        session.add(link)
                        links_created += 1
        
        session.commit()
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {links_created} —Å–≤—è–∑–µ–π –∫–æ–Ω—Ü–µ—Ä—Ç-–∂–∞–Ω—Ä")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_genres = len(genre_map)
        total_concerts = len(concerts)
        
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∂–∞–Ω—Ä–æ–≤:")
        logger.info(f"  ‚Ä¢ –í—Å–µ–≥–æ –∂–∞–Ω—Ä–æ–≤: {total_genres}")
        logger.info(f"  ‚Ä¢ –ö–æ–Ω—Ü–µ—Ä—Ç–æ–≤ —Å –∂–∞–Ω—Ä–∞–º–∏: {total_concerts}")
        logger.info(f"  ‚Ä¢ –°–≤—è–∑–µ–π —Å–æ–∑–¥–∞–Ω–æ: {links_created}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∂–∞–Ω—Ä–æ–≤: {e}")
        session.rollback()
        raise


def load_all_data(session: Session, df_halls: pd.DataFrame, df_concerts: pd.DataFrame, 
                  df_artists: pd.DataFrame, df_details: pd.DataFrame, df_ops: pd.DataFrame,
                  df_off_program: pd.DataFrame = None, df_hall_transitions: pd.DataFrame = None, 
                  disable_fk_checks: bool = True):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
    
    Args:
        session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        df_halls: DataFrame —Å –∑–∞–ª–∞–º–∏
        df_concerts: DataFrame —Å –∫–æ–Ω—Ü–µ—Ä—Ç–∞–º–∏
        df_artists: DataFrame —Å –∞—Ä—Ç–∏—Å—Ç–∞–º–∏
        df_details: DataFrame —Å –¥–µ—Ç–∞–ª—è–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º
        df_ops: DataFrame —Å –ø–æ–∫—É–ø–∫–∞–º–∏
        df_off_program: DataFrame —Å –û—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º–æ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        df_hall_transitions: DataFrame —Å –º–∞—Ç—Ä–∏—Ü–µ–π –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É –∑–∞–ª–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        disable_fk_checks: –û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    """
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    if disable_fk_checks:
        logger.info("–û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏")
        disable_foreign_keys(session)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
        load_halls(session, df_halls)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö –º–µ–∂–¥—É –∑–∞–ª–∞–º–∏ –ø–æ—Å–ª–µ –∑–∞–ª–æ–≤ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã)
        if df_hall_transitions is not None:
            load_hall_transitions(session, df_hall_transitions)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –û—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—É –ø–æ—Å–ª–µ –∑–∞–ª–æ–≤ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–∞)
        if df_off_program is not None:
            load_off_program(session, df_off_program)
        else:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—ç—à –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–∞–∂–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è
            init_off_program_count_cache(session)
        
        load_concerts(session, df_concerts)
        load_artists(session, df_artists)
        load_compositions(session, df_details)
        load_purchases(session, df_ops)
        load_genres(session) # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–∑–æ–≤ load_genres
        
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise
    finally:
        if disable_fk_checks:
            logger.info("–í–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π")
            enable_foreign_keys(session)


def load_routes_from_csv(session: Session, path: str, batch_size: int = 1000, status_dict=None):
    import os
    import logging
    logger = logging.getLogger(__name__)
    if not os.path.exists(path):
        logger.warning(f"[load_routes_from_csv] –§–∞–π–ª {path} –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–∞—Ä—à—Ä—É—Ç–æ–≤.")
        if status_dict is not None:
            status_dict["error"] = f"–§–∞–π–ª {path} –Ω–µ –Ω–∞–π–¥–µ–Ω"
            status_dict["in_progress"] = False
        return {"added": 0, "updated": 0, "skipped": 0}
    added, updated, skipped = 0, 0, 0
    # –°—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    with open(path, newline='', encoding='utf-8') as csvfile:
        import csv
        total_lines = sum(1 for _ in csvfile) - 1  # -1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    if status_dict is not None:
        status_dict["total"] = total_lines
        status_dict["progress"] = 0
        status_dict["added"] = 0
        status_dict["updated"] = 0
        status_dict["error"] = None
    with open(path, newline='', encoding='utf-8') as csvfile:
        import csv
        reader = csv.DictReader(csvfile)
        batch = []
        for i, row in enumerate(reader, 1):
            sostav_raw = row.get('Sostav', '')
            concerts_list = re.findall(r'\d+', sostav_raw)
            concerts_sorted = ','.join(sorted(concerts_list, key=str))
            existing_route = session.exec(select(Route).where(Route.Sostav == concerts_sorted)).first()
            if existing_route:
                for field in Route.__fields__:
                    if field == 'id' or field == 'Sostav':
                        continue
                    value = row.get(field)
                    if value is not None:
                        field_type = Route.__fields__[field].annotation if hasattr(Route, '__fields__') else None
                        if field_type in [int, float]:
                            try:
                                value = field_type(value)
                            except Exception:
                                pass
                        setattr(existing_route, field, value)
                session.add(existing_route)
                updated += 1
            else:
                route_kwargs = {k: v for k, v in row.items()}
                route_kwargs['Sostav'] = concerts_sorted
                for field in Route.__fields__:
                    if field in route_kwargs and route_kwargs[field] is not None:
                        field_type = Route.__fields__[field].annotation if hasattr(Route, '__fields__') else None
                        if field_type in [int, float]:
                            try:
                                route_kwargs[field] = field_type(route_kwargs[field])
                            except Exception:
                                pass
                route = Route(**route_kwargs)
                batch.append(route)
            if len(batch) >= batch_size:
                session.add_all(batch)
                session.commit()
                added += len(batch)
                batch = []
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 —Å—Ç—Ä–æ–∫
            if i % 1000 == 0 or i == total_lines:
                percent = (i / total_lines) * 100 if total_lines > 0 else 100
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{total_lines} –º–∞—Ä—à—Ä—É—Ç–æ–≤ ({percent:.1f}%)")
                if status_dict is not None:
                    status_dict["progress"] = i
                    status_dict["added"] = added
                    status_dict["updated"] = updated
        if batch:
            session.add_all(batch)
            session.commit()
            added += len(batch)
        session.commit()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤
        update_routes_count_cache(session)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º AvailableRoute
        logger.info("–û–±–Ω–æ–≤–ª—è–µ–º AvailableRoute –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞—Ä—à—Ä—É—Ç–æ–≤...")
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º AvailableRoute (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            was_initialized = route_service.ensure_available_routes_exist(session)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats = route_service.get_available_routes_stats(session)
            logger.info(f"AvailableRoute –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {stats['available_routes']} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–∑ {stats['total_routes']} –º–∞—Ä—à—Ä—É—Ç–æ–≤ ({stats['availability_percentage']}%)")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—ç—à–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ AvailableRoute –Ω–µ –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞–Ω–æ–≤–æ
            if not was_initialized:
                route_service.init_available_routes_cache(session)
                route_service.init_available_concerts_cache(session)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ AvailableRoute: {e}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π —Å –º–∞—Ä—à—Ä—É—Ç–∞–º–∏
        logger.info("–û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π —Å –º–∞—Ä—à—Ä—É—Ç–∞–º–∏...")
        try:
            update_customer_route_matches(session)
            logger.info("–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π —Å –º–∞—Ä—à—Ä—É—Ç–∞–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π: {e}")
        
    if status_dict is not None:
        status_dict["progress"] = total_lines
        status_dict["added"] = added
        status_dict["updated"] = updated
        status_dict["in_progress"] = False
    logger.info(f"[load_routes_from_csv] –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–∞—Ä—à—Ä—É—Ç–æ–≤: –¥–æ–±–∞–≤–ª–µ–Ω–æ {added}, –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated}")
    return {"added": added, "updated": updated, "skipped": skipped}


def update_routes_count_cache(session: Session):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü–µ Statistics"""
    try:
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞—Ä—à—Ä—É—Ç–æ–≤
        routes = session.exec(select(Route)).all()
        routes_count = len(routes)
        
        # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é
        stats_record = session.exec(
            select(Statistics).where(Statistics.key == "routes_count")
        ).first()
        
        if stats_record:
            stats_record.value = routes_count
            stats_record.updated_at = datetime.now(timezone.utc)
        else:
            stats_record = Statistics(
                key="routes_count",
                value=routes_count,
                updated_at=datetime.now(timezone.utc)
            )
            session.add(stats_record)
        
        session.commit()
        logger.info(f"–ö—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –æ–±–Ω–æ–≤–ª—ë–Ω: {routes_count}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫—ç—à–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤: {e}")
        session.rollback()


def init_routes_count_cache(session: Session):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—å –≤ –∫—ç—à–µ
        stats_record = session.exec(
            select(Statistics).where(Statistics.key == "routes_count")
        ).first()
        
        if not stats_record:
            # –ï—Å–ª–∏ –∫—ç—à–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞—ë–º –µ–≥–æ
            update_routes_count_cache(session)
            logger.info("–ö—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            logger.info("–ö—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤: {e}")
        session.rollback()


def load_off_program(session: Session, df_off_program: pd.DataFrame):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –û—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã —Ñ–µ—Å—Ç–∏–≤–∞–ª—è
    
    Args:
        session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        df_off_program: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –û—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã
    """
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º {len(df_off_program)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –û—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ Excel")
    
    records = []
    for _, row in df_off_program.iterrows():
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
        format_value = None
        if not pd.isna(row.get("Format")):
            format_str = str(row.get("Format")).strip()
            try:
                format_value = EventFormat(format_str)
            except ValueError:
                logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è: {format_str}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        event_long = str(row.get("EventLong", "00:00:00"))
        if not event_long or event_long == "nan":
            event_long = "00:00:00"
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫—É
        link_value = None
        if not pd.isna(row.get("link")):
            link_value = str(row.get("link")).strip()
            if link_value == "nan":
                link_value = None
        
        records.append({
            "event_num": int(row["EventNum"]),
            "event_name": str(row["EventName"]),
            "description": None if pd.isna(row.get("Description")) else str(row.get("Description")),
            "event_date": pd.to_datetime(row["EventDate"]),
            "event_long": event_long,
            "hall_name": str(row["HallName"]),
            "format": format_value,
            "recommend": bool(row.get("Recommend", False)),
            "link": link_value
        })
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º bulk_get_or_create –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    bulk_get_or_create(session, OffProgram, records, ["event_num"])
    
    count = session.exec(select(OffProgram)).all()
    logger.info(f"–í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å {len(count)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –û—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã
    update_off_program_count_cache(session)


def update_off_program_count_cache(session: Session):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã –≤ —Ç–∞–±–ª–∏—Ü–µ Statistics"""
    try:
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã
        off_program_events = session.exec(select(OffProgram)).all()
        off_program_count = len(off_program_events)
        
        # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é
        stats_record = session.exec(
            select(Statistics).where(Statistics.key == "off_program_count")
        ).first()
        
        if stats_record:
            stats_record.value = off_program_count
            stats_record.updated_at = datetime.now(timezone.utc)
        else:
            stats_record = Statistics(
                key="off_program_count",
                value=off_program_count,
                updated_at=datetime.now(timezone.utc)
            )
            session.add(stats_record)
        
        session.commit()
        logger.info(f"–ö—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã –æ–±–Ω–æ–≤–ª—ë–Ω: {off_program_count}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫—ç—à–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
        session.rollback()


def init_off_program_count_cache(session: Session):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—å –≤ –∫—ç—à–µ
        stats_record = session.exec(
            select(Statistics).where(Statistics.key == "off_program_count")
        ).first()
        
        if not stats_record:
            # –ï—Å–ª–∏ –∫—ç—à–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞—ë–º –µ–≥–æ
            update_off_program_count_cache(session)
            logger.info("–ö—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            logger.info("–ö—ç—à –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –æ—Ñ—Ñ-–ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
        session.rollback()


def load_hall_transitions(session: Session, df_transitions: pd.DataFrame):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É –∑–∞–ª–∞–º–∏
    
    Args:
        session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        df_transitions: DataFrame —Å –º–∞—Ç—Ä–∏—Ü–µ–π –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É –∑–∞–ª–∞–º–∏
    """
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö –º–µ–∂–¥—É –∑–∞–ª–∞–º–∏ –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã {df_transitions.shape}")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ª—ã –≤ –ø–∞–º—è—Ç—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    halls = {hall.name: hall for hall in session.exec(select(Hall)).all()}
    
    # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –æ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö
    session.exec(delete(HallTransition))
    session.commit()
    
    records = []
    transition_count = 0
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
    for _, row in df_transitions.iterrows():
        from_hall_name = row.iloc[0]  # –ü–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ª –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        from_hall = halls.get(from_hall_name)
        if not from_hall:
            logger.warning(f"–ó–∞–ª –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è '{from_hall_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            continue
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –∫–æ–ª–æ–Ω–∫—É (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–π, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∑–∞–ª–æ–≤)
        for col_name, transition_time in row.iloc[1:].items():
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ NaN
            if pd.isna(transition_time) or transition_time == '':
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ª –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            to_hall = halls.get(col_name)
            if not to_hall:
                logger.warning(f"–ó–∞–ª –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è '{col_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–∞–º–æ–º—É —Å–µ–±–µ (–¥–∏–∞–≥–æ–Ω–∞–ª—å –º–∞—Ç—Ä–∏—Ü—ã)
            if from_hall.id == to_hall.id:
                continue
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Ä–µ–º—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ
            try:
                transition_time_int = int(transition_time)
            except (ValueError, TypeError):
                logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞: {transition_time} –¥–ª—è {from_hall_name} -> {col_name}")
                continue
            
            # –°–æ–∑–¥–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä—è–º–æ–π –ø–µ—Ä–µ—Ö–æ–¥ (—ç–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞ –≤ –ë–î)
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –±—É–¥–µ—Ç –Ω–∞–π–¥–µ–Ω —Ñ—É–Ω–∫—Ü–∏–µ–π calculate_transition_time
            records.append({
                "from_hall_id": from_hall.id,
                "to_hall_id": to_hall.id,
                "transition_time": transition_time_int
            })
            
            transition_count += 1  # –î–æ–±–∞–≤–ª—è–µ–º 1 –ø–µ—Ä–µ—Ö–æ–¥
            
            logger.debug(f"–°–æ–∑–¥–∞–Ω –ø–µ—Ä–µ—Ö–æ–¥: {from_hall_name} ‚Üí {col_name} ({transition_time_int} –º–∏–Ω)")
    
    # –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π
    if records:
        session.add_all([HallTransition(**record) for record in records])
        session.commit()
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {transition_count} –∑–∞–ø–∏—Å–µ–π –æ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö –º–µ–∂–¥—É –∑–∞–ª–∞–º–∏")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    total_transitions = session.exec(select(HallTransition)).all()
    logger.info(f"–í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å {len(total_transitions)} –∑–∞–ø–∏—Å–µ–π –æ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö –º–µ–∂–¥—É –∑–∞–ª–∞–º–∏")